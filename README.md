# PromptToSTL

G√©n√©ration de mod√®les 3D (STL/STEP) √† partir de descriptions en langage naturel. Le syst√®me utilise une architecture multi-agent avec des LLMs locaux (Ollama) pour transformer du texte en code CAD ex√©cutable.

## Fonctionnalit√©s

### Templates Optimis√©s (8 types)
G√©n√©ration ultra-rapide (~2s) pour les types courants :
- **M√©dical** : splints (orth√®ses), stents cardiovasculaires
- **M√©canique** : dissipateurs thermiques, pinces robotiques
- **Architecture** : fa√ßades param√©triques, structures en nid d'abeille
- **Structures** : lattices, g√©om√©tries param√©triques

### G√©n√©ration Universelle (Chain-of-Thought)
Pour tout ce qui n'est pas dans les templates (~20s) :
- Engrenages, vis, √©crous
- Supports, brackets, adaptateurs
- Bo√Ætiers, enclosures
- Connecteurs, joints
- **N'importe quelle forme descriptible**

## Architecture

### 12 Agents Sp√©cialis√©s

**Agents Base** (3)
- `AnalystAgent` : D√©tecte le type et extrait les param√®tres
- `GeneratorAgent` : G√©n√®re le code depuis les templates
- `ValidatorAgent` : Ex√©cute et valide le code

**Agents Multi-Agent** (6)
- `OrchestratorAgent` : Coordonne tout le pipeline + routing intelligent
- `DesignExpertAgent` : Valide les r√®gles m√©tier (Qwen2.5-Coder 7B)
- `ConstraintValidatorAgent` : V√©rifie les contraintes de fabrication
- `SyntaxValidatorAgent` : Valide la syntaxe Python avant ex√©cution
- `ErrorHandlerAgent` : Cat√©gorise et g√®re les erreurs
- `SelfHealingAgent` : Corrige automatiquement le code (DeepSeek-Coder 6.7B)

**Agents Chain-of-Thought** (3)
- `ArchitectAgent` : Analyse et raisonne sur le design (Qwen2.5 14B)
- `PlannerAgent` : Cr√©e le plan de construction (Qwen2.5-Coder 14B)
- `CodeSynthesizerAgent` : G√©n√®re le code final (DeepSeek-Coder 33B)

### Routing Intelligent

```
Prompt utilisateur
    ‚Üì
Analyst Agent d√©tecte le type
    ‚Üì
    ‚îú‚îÄ Type connu ‚Üí Template (2s, local)
    ‚îî‚îÄ Type inconnu ‚Üí Chain-of-Thought (20s, local)
```

## Installation

> üìö **Guides D√©taill√©s Disponibles** :
> - **[INSTALL_OLLAMA.md](INSTALL_OLLAMA.md)** - Guide complet d'installation d'Ollama (recommand√© pour d√©butants)
> - **[verify_ollama.sh](verify_ollama.sh)** - Script de v√©rification automatique de l'installation
> - **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - R√©solution des erreurs courantes
> - **[TEST_LLM.md](TEST_LLM.md)** - Tests et cas d'usage

### Pr√©requis
- Python 3.10+
- Ollama
- 16-32 GB RAM (selon les mod√®les)

### 1. Installer Ollama

**Guide rapide** - T√©l√©chargez depuis [ollama.ai](https://ollama.ai)

**Guide complet** - Voir [INSTALL_OLLAMA.md](INSTALL_OLLAMA.md) pour instructions d√©taill√©es

### 2. T√©l√©charger les Mod√®les

**Configuration standard (32GB RAM)** :
```bash
ollama pull qwen2.5-coder:7b
ollama pull deepseek-coder:6.7b
ollama pull qwen2.5:14b
ollama pull qwen2.5-coder:14b
ollama pull deepseek-coder:33b
```

**Configuration l√©g√®re (16GB RAM)** :
```bash
ollama pull qwen2.5-coder:7b
ollama pull deepseek-coder:6.7b
ollama pull qwen2.5:7b
```

### 3. Installer les D√©pendances Python

```bash
pip install -r requirements.txt
```

### 4. Configuration

Copiez `.env.example` vers `.env` et ajustez si n√©cessaire :

```bash
# Ollama
OLLAMA_BASE_URL=http://localhost:11434

# Mod√®les pour agents multi-agent
DESIGN_EXPERT_MODEL=qwen2.5-coder:7b
CODE_LLM_MODEL=deepseek-coder:6.7b

# Mod√®les pour Chain-of-Thought
COT_ARCHITECT_MODEL=qwen2.5:14b
COT_PLANNER_MODEL=qwen2.5-coder:14b
COT_SYNTHESIZER_MODEL=deepseek-coder:33b
```

### 5. Lancer l'Application

```bash
# Terminal 1 : Ollama
ollama serve

# Terminal 2 : Backend
cd backend
python main.py

# Terminal 3 : Frontend (optionnel)
cd frontend
python -m http.server 3000
```

L'API sera disponible sur `http://localhost:8000`

## Utilisation

### API REST

**Endpoint principal** : `POST /api/generate`

```bash
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "create a gear with 20 teeth"}'
```

**R√©ponse** (Server-Sent Events) :
```json
{"type": "status", "message": "Analyzing...", "progress": 10}
{"type": "code", "code": "import cadquery as cq\n...", "progress": 70}
{"type": "complete", "mesh": {...}, "stl_path": "output/gear.stl"}
```

**T√©l√©charger les fichiers** :
- STL : `GET /api/export/stl`
- STEP : `GET /api/export/step`

### Interface Web

Ouvrez `http://localhost:3000` dans votre navigateur.

### Exemples

```python
# Orth√®se m√©dicale (template)
{
  "prompt": "create a wrist splint 270mm long, 70mm wide, 3.5mm thick"
}

# Engrenage (Chain-of-Thought)
{
  "prompt": "create a gear with 20 teeth, 50mm diameter, 10mm thick"
}

# Support personnalis√© (Chain-of-Thought)
{
  "prompt": "create a camera mount bracket for 1/4 inch screw"
}

# Forme simple (Chain-of-Thought)
{
  "prompt": "create a cube 50mm"
}
```

### üöÄ Batch Runner - Ex√©cution Automatique

Ex√©cutez automatiquement plusieurs prompts CAD avec logs et sauvegarde des r√©sultats :

```bash
# M√©thode simple (recommand√©e)
./run_batch.sh

# Ou directement avec Python
python3 batch_runner.py

# Avec fichier de prompts personnalis√©
python3 batch_runner.py mes_prompts.json
```

**Fonctionnalit√©s** :
- ‚úÖ Ex√©cution s√©quentielle de tous les prompts
- üìù Logs complets pour chaque prompt
- üíæ Sauvegarde du code Python g√©n√©r√©
- üìä Rapport JSON avec r√©sultats et m√©triques
- ‚è±Ô∏è Mesure du temps d'ex√©cution

**Personnalisation** : √âditez `prompts.json` pour ajouter vos propres prompts :

```json
{
  "prompts": [
    {
      "id": 1,
      "name": "Mon Objet",
      "enabled": true,
      "prompt": "Create a custom object..."
    }
  ]
}
```

**R√©sultats** : Tous les fichiers sont sauvegard√©s dans `batch_results/` :
- `batch_run_*.log` - Logs d'ex√©cution complets
- `batch_results_*.json` - R√©sultats structur√©s avec tous les d√©tails
- `prompt_*_code.py` - Code Python g√©n√©r√© pour chaque prompt
- Fichiers STL dans `backend/output/`

Voir [BATCH_README.md](BATCH_README.md) pour la documentation compl√®te.

## Performance

| Type | Temps | Co√ªt | Mode |
|------|-------|------|------|
| Template | 1-2s | $0 | Local |
| CoT Simple | 12-15s | $0 | Local |
| CoT Moyen | 18-22s | $0 | Local |
| CoT Complexe | 25-35s | $0 | Local |

*Temps avec mod√®les 14B/33B sur CPU. 2-3x plus rapide avec GPU.*

## Avantages

- ‚úÖ **100% Gratuit** - Aucun frais d'API, mod√®les open-source
- ‚úÖ **100% Local** - Pas de connexion internet requise (apr√®s setup)
- ‚úÖ **100% Priv√©** - Vos donn√©es restent sur votre machine
- ‚úÖ **Illimit√©** - Pas de quota, pas de rate limiting
- ‚úÖ **Flexible** - G√©n√®re n'importe quelle forme 3D

## Technologies

- **Backend** : FastAPI, Python 3.10+
- **CAD** : CadQuery 2.4
- **LLMs** : Ollama (Qwen2.5, DeepSeek-Coder)
- **Export** : STL, STEP

## Documentation

### Architecture et Syst√®me
- [COT_SYSTEM.md](COT_SYSTEM.md) - Documentation du syst√®me Chain-of-Thought
- [MULTI_AGENT_SYSTEM.md](MULTI_AGENT_SYSTEM.md) - Architecture multi-agent d√©taill√©e

### Installation et Configuration
- [INSTALL_OLLAMA.md](INSTALL_OLLAMA.md) - Guide complet d'installation d'Ollama
- [TROUBLESHOOTING.md](TROUBLESHOOTING.md) - R√©solution des probl√®mes courants
- [TEST_LLM.md](TEST_LLM.md) - Tests et validation du syst√®me LLM

## D√©pannage

**Ollama n'est pas accessible**
```bash
# V√©rifiez qu'Ollama tourne
ollama serve

# Test
curl http://localhost:11434/api/tags
```

**Mod√®le non trouv√©**
```bash
ollama pull qwen2.5:14b
```

**M√©moire insuffisante**

Utilisez les mod√®les 7B dans `.env` :
```bash
COT_ARCHITECT_MODEL=qwen2.5:7b
COT_PLANNER_MODEL=qwen2.5-coder:7b
COT_SYNTHESIZER_MODEL=deepseek-coder:6.7b
```

**G√©n√©ration trop lente**

- Utilisez un GPU (2-3x plus rapide)
- Passez aux mod√®les 7B
- Fermez les autres applications

## Contribuer

Les contributions sont bienvenues !

1. Fork le repo
2. Cr√©ez votre branche (`git checkout -b feature/AmazingFeature`)
3. Commit vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Push sur la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

## Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

## Auteurs

- D√©veloppement initial et architecture multi-agent

## Remerciements

- [CadQuery](https://github.com/CadQuery/cadquery) pour le moteur CAD
- [Ollama](https://ollama.ai) pour l'infrastructure LLM locale
- [Qwen](https://github.com/QwenLM/Qwen) et [DeepSeek](https://github.com/deepseek-ai/DeepSeek-Coder) pour les mod√®les open-source
