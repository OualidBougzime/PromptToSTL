# üîß Troubleshooting Guide - PromptToSTL

## ‚ùå Erreur : "Ollama API call failed: All connection attempts failed"

### **Sympt√¥mes**
```
ERROR:cadamx.cot_agents:Ollama CoT API call failed: All connection attempts failed
WARNING:cadamx.cot_agents:Falling back to heuristic mode
ERROR:cadamx.cot_agents:‚ùå Architect JSON parsing failed: Expecting value: line 1 column 1
```

### **Cause**
**Ollama n'est pas lanc√©** ou n'est pas accessible sur `http://localhost:11434`.

---

## ‚úÖ Solutions

### **Solution 1 : Lancer Ollama** (Recommand√©)

#### **√âtape 1 : V√©rifier qu'Ollama est install√©**
```bash
ollama --version
```

Si pas install√© :
- **Windows** : https://ollama.com/download
- **Linux/Mac** : `curl -fsSL https://ollama.com/install.sh | sh`

#### **√âtape 2 : Lancer Ollama (terminal s√©par√©)**
```bash
ollama serve
```

**Attendu** :
```
Ollama is running
```

#### **√âtape 3 : T√©l√©charger les mod√®les LLM**

**Option A : Mod√®les 7B (L√©gers - 13GB total)**
```bash
ollama pull qwen2.5:7b           # 4.7GB - Architect
ollama pull qwen2.5-coder:7b     # 4.7GB - Planner
ollama pull deepseek-coder:6.7b  # 3.8GB - Synthesizer
```

**Option B : Mod√®les 14B/33B (Lourds - 36GB total)**
```bash
ollama pull qwen2.5:14b           # 8.5GB
ollama pull qwen2.5-coder:14b     # 8.5GB
ollama pull deepseek-coder:33b    # 19GB
```

#### **√âtape 4 : Configurer `.env`**

Pour mod√®les 7B (l√©gers) :
```bash
COT_ARCHITECT_MODEL=qwen2.5:7b
COT_PLANNER_MODEL=qwen2.5-coder:7b
COT_SYNTHESIZER_MODEL=deepseek-coder:6.7b
```

Pour mod√®les 14B/33B (configuration par d√©faut) :
```bash
COT_ARCHITECT_MODEL=qwen2.5:14b
COT_PLANNER_MODEL=qwen2.5-coder:14b
COT_SYNTHESIZER_MODEL=deepseek-coder:33b
```

#### **√âtape 5 : V√©rifier la connexion**
```bash
curl http://localhost:11434/api/tags
```

**Attendu** : Liste JSON des mod√®les install√©s

#### **√âtape 6 : Red√©marrer le backend**
```bash
# Tuer le processus Python existant (Ctrl+C)
cd backend
python main.py
```

#### **√âtape 7 : Tester**
```
Prompt: "Generate a torus with major radius 40mm and minor radius 10mm"
```

**Logs attendus** :
```
INFO: üß† Using full LLM pipeline (Fast-path disabled)
INFO: üèóÔ∏è Analyzing: Generate a torus...
INFO: üìê Planning: Simple torus
INFO: üíª Synthesizer: Code generated (confidence: 0.80-0.90)
‚úÖ STL exported to: ...
```

---

### **Solution 2 : Mode Fallback** (Sans Ollama)

Si vous ne pouvez pas lancer Ollama, le syst√®me utilise un **fallback intelligent** qui fonctionne pour les formes simples :

**Formes support√©es en fallback** :
- ‚úÖ Torus (tore)
- ‚úÖ Cylinder (cylindre)
- ‚úÖ Sphere (sph√®re)
- ‚úÖ Cone (c√¥ne)
- ‚úÖ Box (cube)

**Limitations** :
- ‚ùå Pas de g√©n√©ration dynamique (code statique)
- ‚ùå Param√®tres fixes (pas d'extraction du prompt)
- ‚ùå Formes complexes non support√©es (engrenages, brackets, etc.)
- üü° Confiance : 50% (vs 80-90% avec LLM)

Le fallback g√©n√®re du code Python valide mais utilise des **valeurs par d√©faut** :
```python
# Torus fallback
profile = cq.Workplane("XZ").moveTo(40, 0).circle(10)  # Valeurs fixes !
result = profile.revolve(360, (0, 0, 0), (0, 1, 0))
```

---

## üêõ Autres Erreurs Courantes

### **Erreur : "Syntax validation failed"**
```
WARNING:cadamx.multi_agent:‚ö†Ô∏è Syntax Validation failed, retrying...
ERROR:cadamx.multi_agent:‚ùå Healing failed: invalid syntax
```

**Cause** : Le code g√©n√©r√© par le LLM (ou fallback) est syntaxiquement invalide.

**Solutions** :
1. **Avec Ollama** : Le LLM peut halluciner des m√©thodes inexistantes
   - Solution : Am√©liorer les prompts syst√®me (d√©j√† fait)
   - Solution : Ajuster la temp√©rature (r√©duire de 0.3 ‚Üí 0.1 pour Synthesizer)

2. **Sans Ollama (fallback)** : Bug dans le fallback
   - Solution : V√©rifier `cot_agents.py:144-228` (fallback Synthesizer)

---

### **Erreur : "No pending wires present"**
```
ERROR: StdFail_NotDone: TopoDS_Builder::MakeCompound: No pending wires present
```

**Cause** : Tentative de `revolve()` sur un plan incorrect pour le tore.

**Solution** : Utiliser le **plan XZ** (pas XY) pour revolve autour de l'axe Y :
```python
# ‚úÖ CORRECT
profile = cq.Workplane("XZ").moveTo(40, 0).circle(10)
result = profile.revolve(360, (0, 0, 0), (0, 1, 0))

# ‚ùå INCORRECT
profile = cq.Workplane("XY").moveTo(40, 0).circle(10)  # Erreur !
result = profile.revolve(360, (0, 0, 0), (0, 1, 0))
```

---

### **Erreur : "Hallucination LLM" (m√©thodes inexistantes)**
```
ERROR: AttributeError: 'Workplane' object has no attribute 'torus'
ERROR: AttributeError: 'Workplane' object has no attribute 'cone'
```

**Cause** : Le LLM invente des m√©thodes qui n'existent pas en CadQuery.

**Solution** : Le **SelfHealingAgent** corrige automatiquement :
```python
# D√©tecte et corrige :
- .torus() ‚Üí revolve pattern
- .cone() ‚Üí circle + loft
- .regularPolygon() ‚Üí .polygon()
- revolve(angle=360) ‚Üí revolve(360)
- cut() ‚Üí cutThruAll()
```

Si correction √©choue :
- R√©duire la temp√©rature du Synthesizer : `temperature=0.1` (ligne 639)
- Utiliser un mod√®le plus gros : `deepseek-coder:33b`

---

## üìä Comparaison : LLM vs Fallback

| Crit√®re | Avec Ollama (LLM) | Sans Ollama (Fallback) |
|---------|-------------------|------------------------|
| **Formes support√©es** | ‚àû (tout) | 5 (simples) |
| **Extraction param√®tres** | ‚úÖ Oui (du prompt) | ‚ùå Non (valeurs fixes) |
| **G√©n√©ration dynamique** | ‚úÖ Oui | ‚ùå Non (statique) |
| **Confiance** | 80-90% | 50% |
| **Vitesse** | 2-5s | <1s |
| **Requiert Ollama** | ‚úÖ Oui | ‚ùå Non |
| **Requis RAM** | 8-16GB | Minimal |

---

## üéØ Recommandations

### **Pour D√©veloppement**
- ‚úÖ **Utiliser Ollama** avec mod√®les 7B (bon compromis vitesse/qualit√©)
- ‚úÖ Tester d'abord les formes simples (cube, cylindre, sph√®re)
- ‚úÖ Surveiller les logs pour d√©tecter les hallucinations

### **Pour Production**
- ‚úÖ **Utiliser Ollama** avec mod√®les 14B/33B (meilleure qualit√©)
- ‚úÖ Configurer un retry automatique (d√©j√† impl√©ment√© : max 3 tentatives)
- ‚úÖ Monitorer les taux de succ√®s et confiance

### **Pour Tests Rapides**
- üü° Le fallback peut √™tre utilis√© pour des tests basiques
- ‚ö†Ô∏è Mais ne repr√©sente PAS le vrai comportement du syst√®me
- ‚ö†Ô∏è Toujours valider avec Ollama avant d√©ploiement

---

## üìû Support

Si le probl√®me persiste :
1. V√©rifier les logs complets dans le terminal backend
2. V√©rifier que les mod√®les sont bien t√©l√©charg√©s : `ollama list`
3. V√©rifier la RAM disponible : Les mod√®les 14B/33B n√©cessitent 16-32GB
4. Consulter `TEST_LLM.md` pour les cas de test

---

## ‚öôÔ∏è Configuration Avanc√©e

### **Changer l'URL Ollama**
```bash
# .env
OLLAMA_BASE_URL=http://autre-serveur:11434
```

### **Ajuster la temp√©rature LLM**
```python
# cot_agents.py

# Architect (ligne 177)
response = await self.client.generate(messages, temperature=0.5)  # 0.7 ‚Üí 0.5

# Planner (ligne 363)
response = await self.client.generate(messages, temperature=0.3)  # 0.5 ‚Üí 0.3

# Synthesizer (ligne 639)
response = await self.client.generate(messages, temperature=0.1)  # 0.3 ‚Üí 0.1
```

**Plus bas = plus d√©terministe, moins de hallucinations**

### **Augmenter le nombre de retries**
```bash
# .env
MAX_RETRIES=5  # D√©faut : 3
```
