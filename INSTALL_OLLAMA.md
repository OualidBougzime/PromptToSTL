# üöÄ Guide d'Installation Ollama - PromptToSTL

## ‚öôÔ∏è Installation sur Votre Machine Locale

### **Pour Linux** (votre syst√®me actuel)

```bash
# M√©thode 1 : Installation automatique
curl -fsSL https://ollama.com/install.sh | sh

# M√©thode 2 : Installation manuelle (si curl bloqu√©)
# 1. T√©l√©charger depuis https://ollama.com/download/linux
# 2. Extraire et d√©placer le binaire :
sudo mv ollama /usr/local/bin/
sudo chmod +x /usr/local/bin/ollama
```

### **Pour Windows**
- T√©l√©charger l'installateur : https://ollama.com/download/windows
- Ex√©cuter `OllamaSetup.exe`
- Ollama d√©marre automatiquement au d√©marrage de Windows

### **Pour macOS**
```bash
# T√©l√©charger depuis https://ollama.com/download/mac
# Ou via Homebrew :
brew install ollama
```

---

## üöÄ √âtape 1 : Lancer Ollama

**IMPORTANT** : Ollama doit toujours tourner en arri√®re-plan pour que PromptToSTL fonctionne en mode LLM.

### **Terminal 1 : Lancer le serveur Ollama**
```bash
ollama serve
```

**Attendu** :
```
Ollama is running on http://localhost:11434
```

**NOTE** : Laissez ce terminal ouvert ! Ollama doit tourner en permanence.

---

## üì¶ √âtape 2 : T√©l√©charger les Mod√®les LLM

**Ouvrez un NOUVEAU terminal** (le premier doit rester ouvert avec `ollama serve`).

### **Option Recommand√©e : Mod√®les 7B (L√©gers)**

Parfait pour d√©veloppement et tests. N√©cessite **16GB RAM**.

```bash
# Terminal 2
ollama pull qwen2.5:7b           # 4.7GB - Architect (analyse)
ollama pull qwen2.5-coder:7b     # 4.7GB - Planner (planification)
ollama pull deepseek-coder:6.7b  # 3.8GB - Synthesizer (g√©n√©ration code)
```

**Total : ~13GB de t√©l√©chargement**

**Temps estim√©** : 5-15 minutes selon votre connexion

---

### **Option Haute Performance : Mod√®les 14B/33B**

Meilleure qualit√©, mais n√©cessite **32GB RAM** et GPU recommand√©.

```bash
# Terminal 2
ollama pull qwen2.5:14b           # 8.5GB
ollama pull qwen2.5-coder:14b     # 8.5GB
ollama pull deepseek-coder:33b    # 19GB
```

**Total : ~36GB**

---

## ‚úÖ √âtape 3 : V√©rifier l'Installation

### **Test 1 : V√©rifier qu'Ollama r√©pond**
```bash
curl http://localhost:11434/api/tags
```

**Attendu** : JSON avec la liste des mod√®les install√©s
```json
{
  "models": [
    {"name": "qwen2.5:7b", ...},
    {"name": "qwen2.5-coder:7b", ...},
    {"name": "deepseek-coder:6.7b", ...}
  ]
}
```

### **Test 2 : Tester un mod√®le**
```bash
ollama run qwen2.5:7b "Hello, write a Python function to add two numbers"
```

**Attendu** : Le mod√®le g√©n√®re du code Python

---

## üîß √âtape 4 : Configurer PromptToSTL

### **Si vous utilisez les mod√®les 7B (recommand√©)**

Modifiez le fichier `.env` :

```bash
# .env
COT_ARCHITECT_MODEL=qwen2.5:7b
COT_PLANNER_MODEL=qwen2.5-coder:7b
COT_SYNTHESIZER_MODEL=deepseek-coder:6.7b
```

### **Si vous utilisez les mod√®les 14B/33B**

**Aucune modification n√©cessaire**, le `.env` est d√©j√† configur√© pour ces mod√®les :

```bash
COT_ARCHITECT_MODEL=qwen2.5:14b
COT_PLANNER_MODEL=qwen2.5-coder:14b
COT_SYNTHESIZER_MODEL=deepseek-coder:33b
```

---

## üöÄ √âtape 5 : Red√©marrer le Backend

```bash
# Terminal 3 (ou le terminal o√π tourne le backend)
cd /home/user/PromptToSTL/backend
python main.py
```

**Logs attendus au d√©marrage** :
```
INFO: Ollama available at http://localhost:11434
INFO: Chain-of-Thought agents ready (Architect, Planner, Synthesizer)
INFO: Fast-path disabled - using full LLM pipeline
```

---

## üß™ √âtape 6 : Tester la G√©n√©ration Dynamique

Ouvrez l'interface web et testez ces prompts :

### **Test 1 : Formes Simples (anciennement Fast-Path)**
```
1. "Generate a torus with major radius 40mm and minor radius 10mm"
2. "Create a cylinder with radius 25mm and height 60mm"
3. "Make a sphere with radius 30mm"
```

**Logs attendus** (sans "fallback") :
```
INFO: üß† Using full LLM pipeline (Fast-path disabled)
INFO: üèóÔ∏è Architect: Analyzing torus geometry...
INFO: üìê Planner: Planning 4-step construction...
INFO: üíª Synthesizer: Code generated (confidence: 0.85)
‚úÖ STL exported to: output/torus_xxx.stl
```

### **Test 2 : Formes Complexes (impossible en fallback)**
```
4. "Create a hexagonal prism with side 20mm and height 50mm"
5. "Generate a gear with 15 teeth, diameter 50mm, thickness 8mm"
```

**Attendu** : Ces formes devraient maintenant fonctionner (impossible en fallback statique).

---

## üêõ D√©pannage

### **Probl√®me : "Ollama API call failed"**

**Cause** : Ollama n'est pas lanc√© ou pas accessible

**Solution** :
```bash
# V√©rifier qu'Ollama tourne
ps aux | grep ollama

# Si non lanc√©, d√©marrer :
ollama serve
```

### **Probl√®me : "Model not found"**

**Cause** : Mod√®le pas t√©l√©charg√© ou mauvais nom dans `.env`

**Solution** :
```bash
# Lister les mod√®les install√©s
ollama list

# T√©l√©charger le mod√®le manquant
ollama pull qwen2.5:7b
```

### **Probl√®me : "Out of memory"**

**Cause** : Pas assez de RAM pour les mod√®les 14B/33B

**Solution** : Utiliser les mod√®les 7B (modifiez `.env`)

---

## üìä Comparaison : Avant vs Apr√®s Ollama

| Crit√®re | Sans Ollama (Fallback) | Avec Ollama (LLM) ‚úÖ |
|---------|------------------------|---------------------|
| **Formes support√©es** | 5 (torus, cylinder, sphere, cone, box) | ‚àû (toutes) |
| **Extraction param√®tres** | ‚ùå Valeurs fixes | ‚úÖ Du prompt |
| **Code dynamique** | ‚ùå Statique | ‚úÖ G√©n√©r√© par LLM |
| **Confiance** | 50% | 80-90% |
| **Vitesse** | <1s | 2-5s |
| **Formes complexes** | ‚ùå Impossible | ‚úÖ Possible |

---

## üéØ Checklist Finale

Avant de consid√©rer l'installation termin√©e :

- [ ] Ollama install√© (`ollama --version` fonctionne)
- [ ] Ollama lanc√© (`ollama serve` tourne en arri√®re-plan)
- [ ] 3 mod√®les t√©l√©charg√©s (`ollama list` affiche qwen2.5, qwen2.5-coder, deepseek-coder)
- [ ] `.env` configur√© avec les bons noms de mod√®les
- [ ] Backend red√©marr√©
- [ ] Test torus : pas de message "fallback" dans les logs
- [ ] Test forme complexe (hexagonal prism) r√©ussit

---

## üí° Conseils d'Utilisation

### **Quand Ollama Tourne**
- ‚úÖ G√©n√©ration 100% dynamique
- ‚úÖ Extraction automatique des param√®tres
- ‚úÖ Formes complexes support√©es
- ‚úÖ Haute confiance (80-90%)

### **Quand Ollama Est Arr√™t√©**
- üü° Fallback automatique activ√© (s√©curit√©)
- üü° Seulement 5 formes simples
- üü° Valeurs par d√©faut (non extraites du prompt)
- üü° Confiance basse (50%)

**Recommandation** : Toujours lancer `ollama serve` avant de d√©marrer le backend !

---

## üìû Support

**Probl√®mes persistants ?**
- Consultez `TROUBLESHOOTING.md` pour erreurs d√©taill√©es
- Consultez `TEST_LLM.md` pour cas de test complets
- V√©rifiez les logs du backend pour messages d'erreur

**Ressources Ollama** :
- Documentation : https://ollama.com/docs
- Mod√®les disponibles : https://ollama.com/library
- GitHub : https://github.com/ollama/ollama
